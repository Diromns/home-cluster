{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Cluster \u00b6 Welcome to the docs on my home Kubernetes cluster.","title":"Introduction"},{"location":"#home-cluster","text":"Welcome to the docs on my home Kubernetes cluster.","title":"Home Cluster"},{"location":"calico-metrics/","text":"calico metrics \u00b6 calico-node \u00b6 calicoctl patch felixConfiguration default --patch '{\"spec\":{\"prometheusMetricsEnabled\": true}}' kubectl -n calico-system edit ds calico-node Under spec.template.spec.containers : # ... ports : - containerPort : 9091 name : http-metrics protocol : TCP # ... calico-typha \u00b6 kubectl -n calico-system edit deployment calico-typha Under spec.template.spec.containers : # ... - env : - name : TYPHA_PROMETHEUSMETRICSENABLED value : \"true\" - name : TYPHA_PROMETHEUSMETRICSPORT value : \"9092\" # ... ports : - containerPort : 9092 name : http-metrics protocol : TCP # ... calico-kube-controllers \u00b6 This is not working I am unable to patch kubecontrollersconfiguration with the prometheus port calicoctl patch kubecontrollersconfiguration default --patch '{\"spec\":{\"prometheusMetricsPort\": 9094}}' kubectl -n calico-system edit deployment calico-kube-controllers Under spec.template.spec.containers : # ... ports : - containerPort : 9094 name : http-metrics protocol : TCP # ...","title":"calico metrics"},{"location":"calico-metrics/#calico-metrics","text":"","title":"calico metrics"},{"location":"calico-metrics/#calico-node","text":"calicoctl patch felixConfiguration default --patch '{\"spec\":{\"prometheusMetricsEnabled\": true}}' kubectl -n calico-system edit ds calico-node Under spec.template.spec.containers : # ... ports : - containerPort : 9091 name : http-metrics protocol : TCP # ...","title":"calico-node"},{"location":"calico-metrics/#calico-typha","text":"kubectl -n calico-system edit deployment calico-typha Under spec.template.spec.containers : # ... - env : - name : TYPHA_PROMETHEUSMETRICSENABLED value : \"true\" - name : TYPHA_PROMETHEUSMETRICSPORT value : \"9092\" # ... ports : - containerPort : 9092 name : http-metrics protocol : TCP # ...","title":"calico-typha"},{"location":"calico-metrics/#calico-kube-controllers","text":"This is not working I am unable to patch kubecontrollersconfiguration with the prometheus port calicoctl patch kubecontrollersconfiguration default --patch '{\"spec\":{\"prometheusMetricsPort\": 9094}}' kubectl -n calico-system edit deployment calico-kube-controllers Under spec.template.spec.containers : # ... ports : - containerPort : 9094 name : http-metrics protocol : TCP # ...","title":"calico-kube-controllers"},{"location":"external-secrets/","text":"External Secrets \u00b6 Work in progress This document is a work in progress. Create secret for External Secrets using AWS Secrets Manager \u00b6 kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system Create a secret using aws-cli \u00b6 aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"External Secrets"},{"location":"external-secrets/#external-secrets","text":"Work in progress This document is a work in progress.","title":"External Secrets"},{"location":"external-secrets/#create-secret-for-external-secrets-using-aws-secrets-manager","text":"kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system","title":"Create secret for External Secrets using AWS Secrets Manager"},{"location":"external-secrets/#create-a-secret-using-aws-cli","text":"aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"Create a secret using aws-cli"},{"location":"flux/","text":"Flux \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install fluxcd/tap/flux Install the cluster components \u00b6 For full installation guide visit the Flux installation guide Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster set -x GITHUB_TOKEN xyz ; flux bootstrap github \\ --version = v0.12.1 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster/base \\ --personal \\ --private = false \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work Useful commands \u00b6 Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Flux"},{"location":"flux/#flux","text":"Work in progress This document is a work in progress.","title":"Flux"},{"location":"flux/#install-the-cli-tool","text":"brew install fluxcd/tap/flux","title":"Install the CLI tool"},{"location":"flux/#install-the-cluster-components","text":"For full installation guide visit the Flux installation guide Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster set -x GITHUB_TOKEN xyz ; flux bootstrap github \\ --version = v0.12.1 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster/base \\ --personal \\ --private = false \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work","title":"Install the cluster components"},{"location":"flux/#useful-commands","text":"Force flux to sync your repository: flux reconcile source git flux-system Force flux to sync a helm release: flux reconcile helmrelease sonarr -n default Force flux to sync a helm repository: flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Useful commands"},{"location":"restore/","text":"Restoring after a cluster failure or rebuild \u00b6 Restoring Flux state \u00b6 1. Locate cluster GPG key \u00b6 export GPG_TTY = $( tty ) export FLUX_KEY_NAME = \"56k prod cluster (Flux) <email>\" gpg --list-secret-keys \" ${ FLUX_KEY_NAME } \" # pub rsa4096 2021-03-11 [SC] # 772154FFF783DE317KLCA0EC77149AC618D75581 # uid [ultimate] 56k prod cluster (Flux) <email> # sub rsa4096 2021-03-11 [E] export FLUX_KEY_FP = 772154FFF783DE317KLCA0EC77149AC618D75581 2. Verify cluster is ready for Flux \u00b6 flux --kubeconfig = ./kubeconfig check --pre # \u25ba checking prerequisites # \u2714 kubectl 1.21.0 >=1.18.0-0 # \u2714 Kubernetes 1.20.5+k3s1 >=1.16.0-0 # \u2714 prerequisites checks passed 3. Pre-create the flux-system namespace \u00b6 kubectl --kubeconfig = ./kubeconfig create namespace flux-system --dry-run = client -o yaml | kubectl --kubeconfig = ./kubeconfig apply -f - 4. Add the Flux GPG key in-order for Flux to decrypt SOPS secrets \u00b6 gpg --export-secret-keys --armor \" ${ FLUX_KEY_FP } \" | kubectl --kubeconfig = ./kubeconfig create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin 5. Install Flux \u00b6 Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig = ./kubeconfig apply --kustomize = ./cluster/base/flux-system # namespace/flux-system configured # customresourcedefinition.apiextensions.k8s.io/alerts.notification.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/buckets.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/gitrepositories.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/helmcharts.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/helmreleases.helm.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/helmrepositories.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/kustomizations.kustomize.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/providers.notification.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/receivers.notification.toolkit.fluxcd.io created # serviceaccount/helm-controller created # serviceaccount/kustomize-controller created # serviceaccount/notification-controller created # serviceaccount/source-controller created # clusterrole.rbac.authorization.k8s.io/crd-controller-flux-system created # clusterrolebinding.rbac.authorization.k8s.io/cluster-reconciler-flux-system created # clusterrolebinding.rbac.authorization.k8s.io/crd-controller-flux-system created # service/notification-controller created # service/source-controller created # service/webhook-receiver created # deployment.apps/helm-controller created # deployment.apps/kustomize-controller created # deployment.apps/notification-controller created # deployment.apps/source-controller created # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"Kustomization\" in version \"kustomize.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"GitRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" at this point after reconciliation Flux state should be restored. Restoring PVCs using Kasten \u00b6 Recovering from a K10 backup involves the following sequence of actions: 1. Create a Kubernetes Secret, k10-dr-secret, using the passphrase provided while enabling DR \u00b6 kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key = <passphrase> 2. Install a fresh K10 instance \u00b6 Ensure that Flux has correctly deployed K10 to it's namespace kasten-io 3. Provide bucket information and credentials for the object storage location \u00b6 Ensure that Flux has correctly deployed the minio storage profile and that it's accessible within K10 4. Restoring the K10 backup \u00b6 Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job helm install k10-restore kasten/k10restore --namespace = kasten-io \\ --set sourceClusterID = <source-clusterID> \\ --set profile.name = <location-profile-name> 5. Application recovery \u00b6 Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Restore Process"},{"location":"restore/#restoring-after-a-cluster-failure-or-rebuild","text":"","title":"Restoring after a cluster failure or rebuild"},{"location":"restore/#restoring-flux-state","text":"","title":"Restoring Flux state"},{"location":"restore/#1-locate-cluster-gpg-key","text":"export GPG_TTY = $( tty ) export FLUX_KEY_NAME = \"56k prod cluster (Flux) <email>\" gpg --list-secret-keys \" ${ FLUX_KEY_NAME } \" # pub rsa4096 2021-03-11 [SC] # 772154FFF783DE317KLCA0EC77149AC618D75581 # uid [ultimate] 56k prod cluster (Flux) <email> # sub rsa4096 2021-03-11 [E] export FLUX_KEY_FP = 772154FFF783DE317KLCA0EC77149AC618D75581","title":"1. Locate cluster GPG key"},{"location":"restore/#2-verify-cluster-is-ready-for-flux","text":"flux --kubeconfig = ./kubeconfig check --pre # \u25ba checking prerequisites # \u2714 kubectl 1.21.0 >=1.18.0-0 # \u2714 Kubernetes 1.20.5+k3s1 >=1.16.0-0 # \u2714 prerequisites checks passed","title":"2. Verify cluster is ready for Flux"},{"location":"restore/#3-pre-create-the-flux-system-namespace","text":"kubectl --kubeconfig = ./kubeconfig create namespace flux-system --dry-run = client -o yaml | kubectl --kubeconfig = ./kubeconfig apply -f -","title":"3. Pre-create the flux-system namespace"},{"location":"restore/#4-add-the-flux-gpg-key-in-order-for-flux-to-decrypt-sops-secrets","text":"gpg --export-secret-keys --armor \" ${ FLUX_KEY_FP } \" | kubectl --kubeconfig = ./kubeconfig create secret generic sops-gpg \\ --namespace = flux-system \\ --from-file = sops.asc = /dev/stdin","title":"4. Add the Flux GPG key in-order for Flux to decrypt SOPS secrets"},{"location":"restore/#5-install-flux","text":"Due to race conditions with the Flux CRDs you will have to run the below command twice. There should be no errors on this second run. kubectl --kubeconfig = ./kubeconfig apply --kustomize = ./cluster/base/flux-system # namespace/flux-system configured # customresourcedefinition.apiextensions.k8s.io/alerts.notification.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/buckets.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/gitrepositories.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/helmcharts.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/helmreleases.helm.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/helmrepositories.source.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/kustomizations.kustomize.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/providers.notification.toolkit.fluxcd.io created # customresourcedefinition.apiextensions.k8s.io/receivers.notification.toolkit.fluxcd.io created # serviceaccount/helm-controller created # serviceaccount/kustomize-controller created # serviceaccount/notification-controller created # serviceaccount/source-controller created # clusterrole.rbac.authorization.k8s.io/crd-controller-flux-system created # clusterrolebinding.rbac.authorization.k8s.io/cluster-reconciler-flux-system created # clusterrolebinding.rbac.authorization.k8s.io/crd-controller-flux-system created # service/notification-controller created # service/source-controller created # service/webhook-receiver created # deployment.apps/helm-controller created # deployment.apps/kustomize-controller created # deployment.apps/notification-controller created # deployment.apps/source-controller created # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"Kustomization\" in version \"kustomize.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"GitRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" # unable to recognize \"./cluster/base/flux-system\": no matches for kind \"HelmRepository\" in version \"source.toolkit.fluxcd.io/v1beta1\" at this point after reconciliation Flux state should be restored.","title":"5. Install Flux"},{"location":"restore/#restoring-pvcs-using-kasten","text":"Recovering from a K10 backup involves the following sequence of actions:","title":"Restoring PVCs using Kasten"},{"location":"restore/#1-create-a-kubernetes-secret-k10-dr-secret-using-the-passphrase-provided-while-enabling-dr","text":"kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key = <passphrase>","title":"1. Create a Kubernetes Secret, k10-dr-secret, using the passphrase provided while enabling DR"},{"location":"restore/#2-install-a-fresh-k10-instance","text":"Ensure that Flux has correctly deployed K10 to it's namespace kasten-io","title":"2. Install a fresh K10 instance"},{"location":"restore/#3-provide-bucket-information-and-credentials-for-the-object-storage-location","text":"Ensure that Flux has correctly deployed the minio storage profile and that it's accessible within K10","title":"3. Provide bucket information and credentials for the object storage location"},{"location":"restore/#4-restoring-the-k10-backup","text":"Install the helm chart that creates the K10 restore job and wait for completion of the k10-restore job helm install k10-restore kasten/k10restore --namespace = kasten-io \\ --set sourceClusterID = <source-clusterID> \\ --set profile.name = <location-profile-name>","title":"4. Restoring the K10 backup"},{"location":"restore/#5-application-recovery","text":"Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"5. Application recovery"},{"location":"rook-ceph-maintenance/","text":"Rook-Ceph Maintenance \u00b6 Work in progress This document is a work in progress. Accessing volumes \u00b6 Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep plex-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.42.50:/volume1/Data /mnt/nfs List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home Handling crashes \u00b6 Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all Helpful links \u00b6 Common issues kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192.168.42.50:/volume1/Data /mnt/nfs kubectl get pv/(kubectl get pv \\ | grep \"tautulli\" \\ | awk -F' ' '{print $1}') -n media -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' rbd map -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data rm -rf /mnt/data/* tar xvf /mnt/nfs/backups/tautulli.tar.gz -C /mnt/data # chown -R 568:568 /mnt/data/ umount /mnt/data && \\ rbd unmap -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa ceph mgr module enable rook ceph orch set backend rook ceph dashboard set-alertmanager-api-host http://kube-prometheus-stack-alertmanager.monitoring.svc:9093 ceph dashboard set-prometheus-api-host http://kube-prometheus-stack-prometheus.monitoring.svc:9090","title":"Rook-Ceph Maintenance"},{"location":"rook-ceph-maintenance/#rook-ceph-maintenance","text":"Work in progress This document is a work in progress.","title":"Rook-Ceph Maintenance"},{"location":"rook-ceph-maintenance/#accessing-volumes","text":"Sometimes I am required to access the data in the pvc , below is an example on how I access the pvc data for my zigbee2mqtt deployment. First start by scaling the app deployment to 0 replicas: kubectl scale deploy/zigbee2mqtt --replicas 0 -n home Get the rbd image name for the app: kubectl get pv/ ( kubectl get pv | grep plex-config-v1 | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' Exec into the rook-direct-mount toolbox: kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash Create a directory to mount the volume to: mkdir -p /mnt/data Mounting a NFS share This can be useful if you want to move data from or to a nfs share Create a directory to mount the nfs share to: mkdir -p /mnt/nfsdata Mount the nfs share: mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192 .168.42.50:/volume1/Data /mnt/nfs List all the rbd block device names: rbd list --pool replicapool Map the rbd block device to a /dev/rbdX device: rbd map -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Mount the /dev/rbdX device: mount /dev/rbdX /mnt/data At this point you'll be able to access the volume data under /mnt/data , you can change files in any way. Backing up or restoring data from a NFS share Restoring data: rm -rf /mnt/data/* tar xvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data chown -R 568 :568 /mnt/data/ Backing up data: tar czvf /mnt/nfsdata/backups/zigbee2mqtt.tar.gz -C /mnt/data/ . When done you can unmount /mnt/data and unmap the rbd device: umount /mnt/data rbd unmap -p replicapool csi-vol-9a010830-8b0a-11eb-b291-6aaa17155076 Lastly you need to scale the deployment replicas back up to 1: kubectl scale deploy/zigbee2mqtt --replicas 1 -n home","title":"Accessing volumes"},{"location":"rook-ceph-maintenance/#handling-crashes","text":"Sometimes rook-ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthy do the following... # list all the crashes ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all","title":"Handling crashes"},{"location":"rook-ceph-maintenance/#helpful-links","text":"Common issues kubectl -n rook-ceph exec -it (kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath='{.items[0].metadata.name}') bash mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=65536,wsize=65536,hard\" 192.168.42.50:/volume1/Data /mnt/nfs kubectl get pv/(kubectl get pv \\ | grep \"tautulli\" \\ | awk -F' ' '{print $1}') -n media -o json \\ | jq -r '.spec.csi.volumeAttributes.imageName' rbd map -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa \\ | xargs -I{} mount {} /mnt/data rm -rf /mnt/data/* tar xvf /mnt/nfs/backups/tautulli.tar.gz -C /mnt/data # chown -R 568:568 /mnt/data/ umount /mnt/data && \\ rbd unmap -p replicapool csi-vol-2bd198a6-9a7d-11eb-ae97-9a71104156fa ceph mgr module enable rook ceph orch set backend rook ceph dashboard set-alertmanager-api-host http://kube-prometheus-stack-alertmanager.monitoring.svc:9093 ceph dashboard set-prometheus-api-host http://kube-prometheus-stack-prometheus.monitoring.svc:9090","title":"Helpful links"},{"location":"rook-ceph-vol-migration-draft/","text":"k scale deployment/frigate -n home --replicas 0 kubectl get pv/(kubectl get pv | grep \"frigate-config[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 \u00b6 kubectl get pv/(kubectl get pv | grep \"frigate-config-v1[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 \u00b6 Toolbox \u00b6 Access rook direct mount kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash mkdir -p /mnt/ { old,new } rbd map -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 | xargs -I {} mount {} /mnt/old rbd map -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 | xargs -0 -I {} sh -c 'mkfs.ext4 {}; mount {} /mnt/new' cp -rp /mnt/old/. /mnt/new chown -R 568 :568 /mnt/new umount /mnt/old umount /mnt/new rbd unmap -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 && \\ rbd unmap -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","title":"Rook ceph vol migration draft"},{"location":"rook-ceph-vol-migration-draft/#csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685","text":"kubectl get pv/(kubectl get pv | grep \"frigate-config-v1[[:space:]+]\" | awk -F' ' '{print $1}') -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName'","title":"csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685"},{"location":"rook-ceph-vol-migration-draft/#csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","text":"","title":"csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076"},{"location":"rook-ceph-vol-migration-draft/#toolbox","text":"Access rook direct mount kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash mkdir -p /mnt/ { old,new } rbd map -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 | xargs -I {} mount {} /mnt/old rbd map -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076 | xargs -0 -I {} sh -c 'mkfs.ext4 {}; mount {} /mnt/new' cp -rp /mnt/old/. /mnt/new chown -R 568 :568 /mnt/new umount /mnt/old umount /mnt/new rbd unmap -p replicapool csi-vol-e210e08c-80f5-11eb-bb77-f25ddf8c8685 && \\ rbd unmap -p replicapool csi-vol-de945d8c-8fc2-11eb-b291-6aaa17155076","title":"Toolbox"},{"location":"sealed-secrets/","text":"Sealed Secrets \u00b6 Work in progress This document is a work in progress. Install the CLI tool \u00b6 brew install kubeseal Install the cluster components \u00b6 --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false Fetch the Sealed Secrets public certificate \u00b6 kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"Sealed Secrets"},{"location":"sealed-secrets/#sealed-secrets","text":"Work in progress This document is a work in progress.","title":"Sealed Secrets"},{"location":"sealed-secrets/#install-the-cli-tool","text":"brew install kubeseal","title":"Install the CLI tool"},{"location":"sealed-secrets/#install-the-cluster-components","text":"--- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false","title":"Install the cluster components"},{"location":"sealed-secrets/#fetch-the-sealed-secrets-public-certificate","text":"kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"Fetch the Sealed Secrets public certificate"},{"location":"snmp-exporter/","text":"SNMP Exporter \u00b6 Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus Clone and build the snmp-exporter generator \u00b6 sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs Update generator.yml \u00b6 Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%) Get the Cyberpower MIB \u00b6 wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/ Generate the snmp.yml \u00b6 This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"SNMP Exporter"},{"location":"snmp-exporter/#snmp-exporter","text":"Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus","title":"SNMP Exporter"},{"location":"snmp-exporter/#clone-and-build-the-snmp-exporter-generator","text":"sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs","title":"Clone and build the snmp-exporter generator"},{"location":"snmp-exporter/#update-generatoryml","text":"Dealing with configmaps Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%)","title":"Update generator.yml"},{"location":"snmp-exporter/#get-the-cyberpower-mib","text":"wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/","title":"Get the Cyberpower MIB"},{"location":"snmp-exporter/#generate-the-snmpyml","text":"This will create a snmp.yml file which will be needed for the configmap for the snmp-exporter deployment export MIBDIRS = mibs ./generator generate","title":"Generate the snmp.yml"},{"location":"opnsense/bgp/","text":"Opnsense | BGP \u00b6 Work in progress This document is a work in progress.","title":"Opnsense | BGP"},{"location":"opnsense/bgp/#opnsense-bgp","text":"Work in progress This document is a work in progress.","title":"Opnsense | BGP"},{"location":"opnsense/pxe/","text":"Opnsense | PXE \u00b6 Work in progress This document is a work in progress. Setting up TFTP \u00b6 Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"Opnsense | PXE"},{"location":"opnsense/pxe/#opnsense-pxe","text":"Work in progress This document is a work in progress.","title":"Opnsense | PXE"},{"location":"opnsense/pxe/#setting-up-tftp","text":"Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"Setting up TFTP"}]}